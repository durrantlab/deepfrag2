{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01024f08",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook illustrates how to use DeepFrag as an API. For command-line use, see the examples in `./tests/run_tests.sh`.\n",
    "\n",
    "The notebook doesn't describe how to train a new Deefrag model from scratch. Instead, it descsribes how to use an already trained checkpoint for inference, and how to finetune that checkpoint.\n",
    "\n",
    "The `./tests/run_tests.sh` script generates sample checkpoint files. Note that these checkpoints are not well trained and should not be used in production. The checkpoint files are not included in the git repo, but this notebook assumes they exist. Be sure to run `./tests/run_tests.sh` before using this notebook, or change the checkpoint paths to your own files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ff1055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Note: NumExpr detected 36 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collagen.GraphMol requires torch_geometric!\n",
      "1.11.0\n",
      "1.5.10\n"
     ]
    }
   ],
   "source": [
    "# Start by importing the needed libraries\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "from apps.deepfrag.run import DeepFrag\n",
    "from argparse import Namespace\n",
    "\n",
    "# Confirm the torch and lightning versions\n",
    "print(torch.__version__)\n",
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b44fe7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a working directory if needed\n",
    "\n",
    "os.system(\"mkdir -p tmp_working_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a139cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: ProDy is configured: verbosity='none'\n"
     ]
    }
   ],
   "source": [
    "# Create the deepfrag model\n",
    "\n",
    "model = DeepFrag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cae0b0",
   "metadata": {},
   "source": [
    "## Inference on a single protein/ligand complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa0d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the deepfrag parameters. These are the same parameters available through the commandline.\n",
    "\n",
    "args = Namespace(\n",
    "    mode=\"inference\",\n",
    "    receptor=\"./tests/data_for_inference/5VUP_prot_955.pdb\",\n",
    "    branch_atm_loc_xyz=\"12.413000, 3.755000, 59.021999\",\n",
    "    ligand=\"./tests/data_for_inference/5VUP_lig_955.sdf\",\n",
    "    load_checkpoint=\"./tests/1.train_on_moad.output/last.ckpt\",\n",
    "    default_root_dir=\"./tmp_working_dir/\",\n",
    "    rotations=2,\n",
    "    inference_label_sets=\"./tests/data_for_inference/label_set.smi\",\n",
    "    num_inference_predictions=10,\n",
    "    fragment_representation=\"rdk10\",\n",
    "    split_seed=-1,\n",
    "    \n",
    "    # Boilerplate\n",
    "    log_every_n_steps=25,\n",
    "    num_dataloader_workers=32,\n",
    "    cache_pdbs_to_disk=True,\n",
    "    learning_rate=0.0001,\n",
    "    gpus=1,\n",
    "    cpu=False,\n",
    "    verbose=True,\n",
    "    aggregation_rotations=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f076f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the fingerprint scheme. This makes some updates to the args depending on whether you're using\n",
    "# rdk10 or molbert fingerprints.\n",
    "\n",
    "model.setup_fingerprint_scheme(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4102d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from checkpoint: ./tests/1.train_on_moad.output/last.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "\n",
    "ckpt = model.load_checkpoint(args, validate_args=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd49a5ae",
   "metadata": {},
   "source": [
    "### Using the `run_inference()` helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bb28964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the operator mean to aggregate the inferences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: 7165 atoms and 1 coordinate set(s) were parsed in 0.08s.\n",
      "INFO: Created a temporary directory at /tmp/tmpxai744w3\n",
      "INFO: Writing /tmp/tmpxai744w3/_remote_module_non_sriptable.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint ./tests/1.train_on_moad.output/last.ckpt\n",
      "\n",
      "Loading model from checkpoint ./tests/1.train_on_moad.output/last.ckpt\n",
      "\n",
      "    Rotation #1\n",
      "    Rotation #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/workspace/users/durraj/deepfrag2/collagen/core/molecules/mol.py:443: UserWarning: Internal rdmol has no conformers\n",
      "  warnings.warn(\"Internal rdmol has no conformers\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label set size: 556\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd715072b2b4d08bbb0ae597733b528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Most Similar Matches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Deepfrag includes a helper function that runs inference, calculates the\n",
    "# average output vector over multiple rotations, and selects the top-K most\n",
    "# similar fragments.\n",
    "\n",
    "result = model.run_inference(args, ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4f9d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['*CC', 0.6488900184631348],\n",
       " ['*CCN', 0.6217541098594666],\n",
       " ['*C(C)N', 0.5775389075279236],\n",
       " ['*CC#C', 0.5486630797386169],\n",
       " ['*CC=C', 0.5315735936164856],\n",
       " ['*CCC', 0.5299135446548462],\n",
       " ['*CCCN', 0.5287677049636841],\n",
       " ['*C(C)C', 0.5200974345207214],\n",
       " ['*CCO', 0.5113264918327332],\n",
       " ['*C(C)O', 0.4969644546508789]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The most similar fragments from your label set\n",
    "\n",
    "result[\"most_similar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c709b722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-rotation fingerprint: tensor([[9.8529e-04, 1.1870e-04, 1.1531e-03,  ..., 3.9976e-05, 1.3216e-04,\n",
      "         8.1263e-01]], grad_fn=<SigmoidBackward0>)\n",
      "\n",
      "Total fingerprints: 2\n"
     ]
    }
   ],
   "source": [
    "# The Deepfrag output fingerprints, per rotation\n",
    "\n",
    "print(\"First-rotation fingerprint:\", result[\"fps\"][\"per_rot\"][0])\n",
    "print(\"\")\n",
    "print(\"Total fingerprints:\", len(result[\"fps\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b37116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fingerprint: tensor([[5.8259e-04, 6.6963e-05, 7.0159e-04,  ..., 2.2288e-05, 7.3835e-05,\n",
      "         8.2951e-01]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# The average fingerprint over all rotations\n",
    "\n",
    "print(\"Average fingerprint:\", result[\"fps\"][\"avg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcfe075",
   "metadata": {},
   "source": [
    "### Without the `run_inference()` helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9022cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: 7165 atoms and 1 coordinate set(s) were parsed in 0.07s.\n"
     ]
    }
   ],
   "source": [
    "# You don't have to use the helper function if you'd\n",
    "# like more customized use. Here is an example.\n",
    "\n",
    "# First, load the receptor and ligand\n",
    "\n",
    "import prody\n",
    "from rdkit import Chem\n",
    "from collagen.core.molecules.mol import Mol\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "\n",
    "# Load the receptor\n",
    "with open(args.receptor, \"r\") as f:\n",
    "    m = prody.parsePDBStream(StringIO(f.read()), model=1)\n",
    "prody_mol = m.select(\"all\")\n",
    "recep = Mol.from_prody(prody_mol)\n",
    "\n",
    "# Load the ligand\n",
    "suppl = Chem.SDMolSupplier(str(args.ligand))\n",
    "rdmol = [x for x in suppl if x is not None][0]\n",
    "lig = Mol.from_rdkit(rdmol, strict=False)\n",
    "\n",
    "# Calculate the center of the receptor. \n",
    "center = np.array([float(v.strip()) for v in args.branch_atm_loc_xyz.split(\",\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f199b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voxelize the receptor and ligand\n",
    "\n",
    "from collagen.util import rand_rot\n",
    "\n",
    "# Get a random rotation\n",
    "rot = rand_rot()\n",
    "\n",
    "# Get the parameters to use when voxelizing the protein and ligand.\n",
    "voxel_params = model.init_voxel_params(args)\n",
    "\n",
    "# Voxelize the receptor\n",
    "recep_vox = recep.voxelize(\n",
    "    voxel_params, cpu=True, center=center, rot=rot\n",
    ")\n",
    "\n",
    "# Voxelize the ligand\n",
    "lig_vox = lig.voxelize(voxel_params, cpu=True, center=center, rot=rot)\n",
    "\n",
    "# Stack the receptor and ligand tensors\n",
    "num_features = recep_vox.shape[1] + lig_vox.shape[1]\n",
    "dimen1 = lig_vox.shape[2]\n",
    "dimen2 = lig_vox.shape[3]\n",
    "dimen3 = lig_vox.shape[4]\n",
    "vox = torch.cat([recep_vox[0], lig_vox[0]]).reshape(\n",
    "    [1, num_features, dimen1, dimen2, dimen3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a51f2ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model from checkpoint ./tests/1.train_on_moad.output/last.ckpt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepFragModel(\n",
       "  (encoder): Sequential(\n",
       "    (0): BatchNorm3d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Conv3d(10, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "    (2): ReLU()\n",
       "    (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): Aggregate3x3Patches(output_size=(1, 1, 1))\n",
       "    (16): Flatten(start_dim=1, end_dim=-1)\n",
       "    (17): Dropout(p=0.5, inplace=False)\n",
       "    (18): Linear(in_features=64, out_features=512, bias=True)\n",
       "    (19): ReLU()\n",
       "  )\n",
       "  (deepfrag_after_encoder): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (2): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "\n",
    "# Set the device (e.g., cuda)\n",
    "device = model.init_device(args)\n",
    "\n",
    "model_initialized = model.init_model(args, ckpt)\n",
    "\n",
    "model_initialized.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4cad495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.9241e-04, 5.0444e-05, 5.8798e-04,  ..., 1.5905e-05, 5.2405e-05,\n",
       "         8.2096e-01]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the appropriate fingerprint given these voxel inputs.\n",
    "\n",
    "fp = model_initialized.forward(vox)\n",
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8b612fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label set size: 556\n"
     ]
    }
   ],
   "source": [
    "# To find the closest fragments to the output fingerprint, we must\n",
    "# prepare our inference label set. There's a helper function for that:\n",
    "\n",
    "(\n",
    "    label_set_fingerprints,\n",
    "    label_set_entry_infos,\n",
    ") = model.create_inference_label_set(\n",
    "    args,\n",
    "    device,\n",
    "    [l.strip() for l in args.inference_label_sets.split(\",\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c12a3e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece6075d7a5e495987185d4ed3ec2e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Most Similar Matches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[['*CC', 0.64890056848526],\n",
       "  ['*CCN', 0.6202546954154968],\n",
       "  ['*C(C)N', 0.5759989619255066],\n",
       "  ['*CC#C', 0.5487274527549744],\n",
       "  ['*CC=C', 0.5310753583908081],\n",
       "  ['*CCC', 0.5292601585388184],\n",
       "  ['*CCCN', 0.5268192291259766],\n",
       "  ['*C(C)C', 0.5198426246643066],\n",
       "  ['*CCO', 0.5128037929534912],\n",
       "  ['*C(C)O', 0.49837005138397217]]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now use another helper function to find the most similar matches.\n",
    "\n",
    "from collagen.metrics.metrics import most_similar_matches\n",
    "\n",
    "most_similar = most_similar_matches(\n",
    "    fp,\n",
    "    label_set_fingerprints,\n",
    "    label_set_entry_infos,\n",
    "    args.num_inference_predictions,  # self.NUM_MOST_SIMILAR_PER_ENTRY,\n",
    ")\n",
    "\n",
    "most_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176af50e",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "When you train a Deepfrag model from scratch (using the CLI), it produces not only a `ckpt` file, but also a `pt` file. The `pt` file is useful for finetuning a Deepfrag model.\n",
    "\n",
    "Note that finetuning is also called \"warm starting.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89cf4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must define new arguments for this new task.    \n",
    "    \n",
    "args = Namespace(\n",
    "    mode=\"warm_starting\",\n",
    "    model_for_warm_starting=\"./tests/1.train_on_moad.output/model_mean_mean_train.pt\",    \n",
    "    default_root_dir=\"./tmp_working_dir/\",\n",
    "    max_epochs=5,\n",
    "    fragment_representation=\"rdk10\",\n",
    "    butina_cluster_cutoff=0.4,\n",
    "    split_seed=-1,\n",
    "    save_params=\"./tmp_working_dir/params.saved.json\",\n",
    "    save_splits=\"./tmp_working_dir/splits.saved.json\",\n",
    "    \n",
    "    # Directory contains protein/ligands named like 1XDN_prot_123.pdb, 1XDN_lig_123.sdf\n",
    "    data_dir=\"./tests/data_to_finetune/\",\n",
    "    \n",
    "    # In this case, let's have no validation set.\n",
    "    fraction_val=0.0,\n",
    "    fraction_train=0.8,\n",
    "    \n",
    "    # Boilerplate\n",
    "    log_every_n_steps=25,\n",
    "    num_dataloader_workers=32,\n",
    "    cache_pdbs_to_disk=True,\n",
    "    learning_rate=0.0001,\n",
    "    gpus=1,\n",
    "    cpu=False,\n",
    "    verbose=True,\n",
    "    aggregation_rotations=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa9153de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'load_splits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_warm_starting\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/workspace/users/durraj/deepfrag2/collagen/model_parents/moad_voxel/train.py:39\u001b[0m, in \u001b[0;36mMoadVoxelModelTrain.run_warm_starting\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_warm_starting\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[1;32m     38\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_trainer(args)\n\u001b[0;32m---> 39\u001b[0m     moad, train_data, val_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_val_sets\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_warm_model(args)\n\u001b[1;32m     43\u001b[0m     model_stats \u001b[38;5;241m=\u001b[39m summary(model, (\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m24\u001b[39m), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/gpfs/workspace/users/durraj/deepfrag2/collagen/model_parents/moad_voxel/train.py:103\u001b[0m, in \u001b[0;36mMoadVoxelModelTrain.get_train_val_sets\u001b[0;34m(self, args, train)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarm_starting\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m mode is not required to specify the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--every_csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     97\u001b[0m train, val, _ \u001b[38;5;241m=\u001b[39m compute_dataset_split(\n\u001b[1;32m     98\u001b[0m     moad,\n\u001b[1;32m     99\u001b[0m     seed\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msplit_seed,\n\u001b[1;32m    100\u001b[0m     fraction_train\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mfraction_train,\n\u001b[1;32m    101\u001b[0m     fraction_val\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mfraction_val,\n\u001b[1;32m    102\u001b[0m     save_splits\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msave_splits,\n\u001b[0;32m--> 103\u001b[0m     load_splits\u001b[38;5;241m=\u001b[39m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_splits\u001b[49m,\n\u001b[1;32m    104\u001b[0m     max_pdbs_train\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_pdbs_train,\n\u001b[1;32m    105\u001b[0m     max_pdbs_val\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_pdbs_val,\n\u001b[1;32m    106\u001b[0m     max_pdbs_test\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_pdbs_test,\n\u001b[1;32m    107\u001b[0m     butina_cluster_cutoff\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbutina_cluster_cutoff,\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# pr = cProfile.Profile()\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# pr.enable()\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# ps.print_stats()\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# open('cProfilez.txt', 'w+').write(s.getvalue())\u001b[39;00m\n\u001b[1;32m    119\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_data_from_split(\n\u001b[1;32m    120\u001b[0m     cache_file\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mcache, \n\u001b[1;32m    121\u001b[0m     args\u001b[38;5;241m=\u001b[39margs, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m    126\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'load_splits'"
     ]
    }
   ],
   "source": [
    "model.run_warm_starting(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adfa747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "\n",
    "trainer = model.init_trainer(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
